{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model\n",
    "%load_ext lab_black\n",
    "# nb_black if running in jupyter\n",
    "%load_ext autoreload\n",
    "# automatically reload python modules if there are changes in the\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\r\n",
    "\r\n",
    "> In this notebook you create and test a Python class to hold your machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***input***: toy dataset from data-notebook\r\n",
    "\r\n",
    "***output***: python module containing ML model class \r\n",
    "\r\n",
    "***description:***\r\n",
    "\r\n",
    "In this notebook you hypothetize, explain and explore machine learning models to solve your problem.\r\n",
    "\r\n",
    "Then, you should encapsulate the model inside a Python class to be exported into `your_repository/your_module/model.py`,\r\n",
    "so that it can be evaluated in the loss notebook, and intergrated with your target application.\r\n",
    "Repository name and module name are the same by default.\r\n",
    "You should also unit test the classes created in this notebook with the toy data created in data notebook.\r\n",
    "\r\n",
    "You should probably have a person in your team familiar with object oriented programming with python and unit testing, but if not, don't worry.\r\n",
    "If you can explore by scripting in the cells and create a draft of the properties and functions you want to have, and that's a great start.\r\n",
    "Then, any Python developer can easily build the model class for you.\r\n",
    "However, we encourage you to learn more on [object oriented programming with Python](https://realpython.com/python3-object-oriented-programming/)\r\n",
    "and [getting started with unit testing in Python](https://realpython.com/python-testing/).\r\n",
    "You can also follow the example here to create your own simple machine learning Python class.\r\n",
    "\r\n",
    "This notebook contains an example ML model for classifying the heart disease dataset with logistic regression.\r\n",
    "\r\n",
    "The example is split into a base class and a subclass for demonstrating class inheritance of Python.\r\n",
    "You can probably just write one class that contains all the attributes and functions you need, without inheriting anything.\r\n",
    "However, alternative implementations of a model might be implemented in separate subclasses, for example.\r\n",
    "You can also define multiple classes, for example one for ML model and another for optimization.\r\n",
    "If the methods are complicated or you are comparing multiple methods that don't share common functions, \r\n",
    "you can also separate models or subclasses to different notebooks similar to this.\r\n",
    "Adjust the running number, name, header and top cell `#default_exp module_name` of the notebooks accordingly.\r\n",
    "\r\n",
    "Remember to add `# export` to top of all cells containing functions or classes that you have defined and want to use outside this notebook.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom modules\n",
    "\n",
    "# plot functions from data notebook\n",
    "from ml_project_template.data import plot_trellis, plot_histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters\r\n",
    "\r\n",
    "Remember, only simple assignments here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag this cell with 'parameters'\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make immediate derivations from the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import toy data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1age</th>\n      <th>x4trestbps</th>\n      <th>x8thalach</th>\n      <th>x10oldpeak</th>\n      <th>y1num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>53.0</td>\n      <td>142.0</td>\n      <td>111.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>56.0</td>\n      <td>200.0</td>\n      <td>133.0</td>\n      <td>4.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>62.0</td>\n      <td>140.0</td>\n      <td>143.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53.0</td>\n      <td>160.0</td>\n      <td>122.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54.0</td>\n      <td>160.0</td>\n      <td>163.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   x1age  x4trestbps  x8thalach  x10oldpeak  y1num\n0   53.0       142.0      111.0         0.0      0\n1   56.0       200.0      133.0         4.0      1\n2   62.0       140.0      143.0         0.0      1\n3   53.0       160.0      122.0         0.0      1\n4   54.0       160.0      163.0         0.0      0"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df = pd.read_csv(\n",
    "    \"data/preprocessed_data/dataset_toy_switzerland_cleveland.csv\", index_col=0\n",
    ")\n",
    "toy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note, that depending on your choice of file format and your variables, you might have to redefine data types once you load data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the math behind\r\n",
    "\r\n",
    "Give a short explanation of how the algorithms work that you are planning to use.\r\n",
    "\r\n",
    "For industrial use, you can keep this light and simple: you can provide links to external sources for further reading.\r\n",
    "\r\n",
    "For research, you might want to dig deeper - this is your core documentation, after all!\r\n",
    "\r\n",
    "You can use $\\LaTeX$ notation to write math symbols and equations:\r\n",
    "\r\n",
    "$$\r\n",
    "Pr(Y_i=1|X_i) = {\\frac{exp(\\beta_0 + \\beta_1X_i + \\dots + \\beta_nX_n)}{1 + exp (\\beta_0 + \\beta_1X_i + \\dots + \\beta_nX_n)}}\r\n",
    "$$\r\n",
    "\r\n",
    "You can also draft algorithms:\r\n",
    "\r\n",
    "    ALGORITHM\r\n",
    "    input: X\r\n",
    "    output: y\r\n",
    "\r\n",
    "    while: condition\r\n",
    "        do thing\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin with simple scripts before constructing the model class\r\n",
    "\r\n",
    "This is the place where you can explore and play around with different machine learning operations.\r\n",
    "Your goal is to plan and demonstrate the features and functions you want your machine learning class to have.\r\n",
    "The good thing is, that you don't need to think about object oriented programming here.\r\n",
    "Just assign variables and call functions. \r\n",
    "\r\n",
    "It's good to define at least the following steps:\r\n",
    "\r\n",
    "1. Splitting data into training and testing data\r\n",
    "2. Preprocess the data (scale, dimension reduction, convolutions etc.)\r\n",
    "3. Define your model algorithm and fit it with toy data\r\n",
    "4. Define your loss function - how do you evaluate your model?\r\n",
    "5. Consider hyperparameter optimization\r\n",
    "6. Try to pipe the previous steps\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by splitting our data to train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data\n",
    "X = toy_df.iloc[:, :-1]\n",
    "y = toy_df.iloc[:, -1]\n",
    "\n",
    "k = 5  # test size 1/k\n",
    "\n",
    "# train_test_split does the job here\n",
    "\n",
    "# stratify = y means, that both the testing and training dataset contain\n",
    "# label values in same proportion as they are in the whole dataset\n",
    "# it only works with categorical data, and might cause errors there\n",
    "# if some category is rare\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1 / k, random_state=seed, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "# create preprocessing steps, model and pipe\n",
    "\n",
    "# preprocessing: let's rescale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# algorithm: logistic regression\n",
    "lgr = LogisticRegression()\n",
    "\n",
    "# pipe: let's join these two in a pipeline\n",
    "# now the pipeline actually becomes our machine learning model:\n",
    "# we can just call the pipe instead of first scaling data and then\n",
    "# calling the algorithm functions separatedly\n",
    "pipe = Pipeline([(\"scaler\", scaler), (\"estimator\", lgr)])\n",
    "# fit data\n",
    "pipe.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    ")\n",
    "# get test score\n",
    "print(pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.5714285714285715"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define loss\n",
    "\n",
    "# let's use f1  for loss here\n",
    "f1_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we optimize the model hyperparameters with cross validated grid search.\r\n",
    "Notice that this is just an example: grid search is very ineffective and does not quarantee good optimization solutions for hyperparameters.\r\n",
    "In real applications Bayesian optimization is usually a great solution.\r\n",
    "You can skip optimization in the beginning and return to it in the later iterations of the project.\r\n",
    "For example sklearn models usually have good default hyperparameters, and you may well get started with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: \n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator', LogisticRegression(C=0.3593813663804626))])\n",
      "\n",
      "Best Parameters: \n",
      "{'estimator__C': 0.3593813663804626}\n",
      "\n",
      "Best Test Score: \n",
      "0.75\n",
      "\n",
      "Best Training Score: \n",
      "0.8375\n",
      "\n",
      "All Training Scores: \n",
      "[0.6    0.6    0.6    0.65   0.8375 0.8375 0.8375 0.8375 0.8375 0.8375]\n",
      "\n",
      "All Test Scores: \n",
      "[0.6  0.6  0.6  0.55 0.75 0.75 0.65 0.65 0.65 0.65]\n",
      "\n",
      "Pipe test score with optimized hyperparams\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "# create optimization pipe\n",
    "\n",
    "## hyperparameters to be optimized (be consistent with the naming)\n",
    "param_grid = {\n",
    "    \"estimator__C\": np.logspace(-4, 4, 10),\n",
    "}\n",
    "\n",
    "# create cross validation for grid search\n",
    "cv = StratifiedKFold(n_splits=k)\n",
    "\n",
    "# create optimizer\n",
    "gs = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=cv,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "# fit optimizer\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# view results\n",
    "print(\"Best Estimator: \\n{}\\n\".format(gs.best_estimator_))\n",
    "print(\"Best Parameters: \\n{}\\n\".format(gs.best_params_))\n",
    "print(\"Best Test Score: \\n{}\\n\".format(gs.best_score_))\n",
    "print(\n",
    "    \"Best Training Score: \\n{}\\n\".format(\n",
    "        gs.cv_results_[\"mean_train_score\"][gs.best_index_]\n",
    "    )\n",
    ")\n",
    "print(\"All Training Scores: \\n{}\\n\".format(gs.cv_results_[\"mean_train_score\"]))\n",
    "print(\"All Test Scores: \\n{}\\n\".format(gs.cv_results_[\"mean_test_score\"]))\n",
    "\n",
    "# set pipe hyperparameters to the optimized:\n",
    "pipe.set_params(estimator__C=gs.best_params_[\"estimator__C\"])\n",
    "\n",
    "# fit pipe\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"Pipe test score with optimized hyperparams\")\n",
    "print(pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we can fit a model to the data and it appears to do something (with the small test data we can not necessarily say if it's meaningful).\r\n",
    "\r\n",
    "However, how would you take this script into production?\r\n",
    "How would you scale it, or use it with completely different setup of data and parameters?\r\n",
    "(well, this is a tiny example, so we could actually easily parameterize a script, but that's rarely the case in real world applications)\r\n",
    "\r\n",
    "This is why we need to construct a model class, to hold all of the steps required in separate, tidy functions.\r\n",
    "Then we can recreate the model and the steps with different data, without copy-pasting or manually editing all the tiny details.\r\n",
    "\r\n",
    "Follow along the example - you'll see, that half the work was done in the scripting cells above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define base class for your ML model\r\n",
    "\r\n",
    "You will probably do just fine creating a one simple class that does not inherit anything.\r\n",
    "You can then use this example base class as a template for your machine learning class. \r\n",
    "However, this is not always the case, and class inheritance is one of the most useful features of Python (and object oriented programming).\r\n",
    "This is why we wanted to demonstrate a base class - subclass division.\r\n",
    "\r\n",
    "Here we define the base class `MachineLearningModel` that holds some simple functions for handling data, that would be common for all subclasses.\r\n",
    "If a function only contains a `pass`-statement, it will be defined in the subclass.\r\n",
    "\r\n",
    "> **Note**: in this example the model instance contains the data. \r\n",
    "This is rarely applicable in practice if the data is large.\r\n",
    "Instead, in most applications the model should be routed to query the data when needed, in a similar way that it would appear as if the model instance contained the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Define base class for our classifier\n",
    "class MachineLearningModel:\n",
    "    \"\"\"\n",
    "    Overly simplified example for a base class:\n",
    "\n",
    "    data handling operations\n",
    "\n",
    "    handle definitions of other functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, n_splits=5, seed=0):\n",
    "        self.n_splits = n_splits  # k-fold n_splits\n",
    "        self.seed = seed  # random state\n",
    "\n",
    "        self.set_data(X, y)  # init model data (see below)\n",
    "\n",
    "    def set_data(self, X, y):\n",
    "        \"\"\"\n",
    "        Set traing and evaluation data\n",
    "        \"\"\"\n",
    "        self.X = X.copy()\n",
    "        self.y = y.copy()\n",
    "\n",
    "        # in addition we separate train and test data:\n",
    "        self.__create_train_test_data()  # see below\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_data(self) -> (np.ndarray, np.ndarray):\n",
    "        \"\"\"\n",
    "        Get training and evaluation data\n",
    "        \"\"\"\n",
    "        return self.X.copy(), self.y.copy()\n",
    "\n",
    "    def __create_train_test_data(self, n_splits=None, seed=None):\n",
    "        \"\"\"\n",
    "        Create training and testing data\n",
    "        \"\"\"\n",
    "        # you might want to control the seed:\n",
    "        if seed is None:\n",
    "            seed = self.seed\n",
    "\n",
    "        # you might want to control the number of splits\n",
    "        if n_splits is None:\n",
    "            n_splits = self.n_splits\n",
    "\n",
    "        # split train and test data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=(1 / n_splits), random_state=seed, stratify=self.y\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_train_test_data(self):\n",
    "        \"\"\"\n",
    "        Return X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "\n",
    "    def fit(self, X=None, y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_train_loss(self):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_test_loss(self):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit test base class\r\n",
    "\r\n",
    "Purpose of unit testing is to cover all possible excecution paths in code.\r\n",
    "Unit testing helps developers to avoid and identify bugs in code.\r\n",
    "\r\n",
    "To unit test the class, we want to try and call every function of it and execute every different possible execution path.\r\n",
    "The proportion of possible paths covered is called the test coverage.\r\n",
    "100% coverage is rarely possible,\r\n",
    "but it would be good to try and test the functions with good and bad input,\r\n",
    "and with possible limit values (min, max, zero, None, np.nan, empty list etc.).\r\n",
    "\r\n",
    "However, a few simple tests are easier to maintain (and to actually get done in the first place) than exhaustive unit testing.\r\n",
    "Don't worry about coverage too much!\r\n",
    "A few well considered tests are better than having none.\r\n",
    "\r\n",
    "At the moment, nbdev considers all cells that do not have `# export` tag as tests.\r\n",
    "Unit tests can be defined with `assert` command - the nbdev git hooks run these commands when you push commits.\r\n",
    "Cells with `# slow` -tag will be omitted for time savings.\r\n",
    "\r\n",
    "The line after `assert` should have a `True` or non-zero value.\r\n",
    "`False`, 0 or None object will raise an `AssertionError`. Note that `np.nan` does not raise the error.\r\n",
    "\r\n",
    "This is handy because you can now keep all your tests in the same file (notebook) with the code and documentation.\r\n",
    "The downside is, however, that at the moment there is no good solution for monitoring test coverage of notebook developed code. \r\n",
    "If test coverage measuring is required, one option would be to implement tests with  `pytest` or `unittest` and export the tests to separate test.py file.\r\n",
    "\r\n",
    "Let's begin by introducing a couple of super simple unit test examples:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a simple unit test\n",
    "this_statement_is_true = True\n",
    "assert this_statement_is_true\n",
    "\n",
    "# Another example of simple unit test of a function\n",
    "def return_three():\n",
    "    return 3\n",
    "\n",
    "\n",
    "# unit test return_tree\n",
    "assert return_three() == 3\n",
    "\n",
    "# Third example of simple unit testing of a simple class\n",
    "class SimpleClass:\n",
    "    \"\"\"\n",
    "    Simple class that stores an attribute and has a function to return it\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parameter):\n",
    "        self.attribute = parameter\n",
    "\n",
    "    def get_attribute(self):\n",
    "        return self.attribute\n",
    "\n",
    "\n",
    "# unit thest init (should return class instance)\n",
    "assert SimpleClass(\"hello world!\")\n",
    "# unit test get_attribute (should return 'Hello world!')\n",
    "assert SimpleClass(\"Hello world!\").get_attribute() == \"Hello world!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the assert commands run without an error, the tests pass.\r\n",
    "\r\n",
    "Now, let's include some tests with our example ML base class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test init\n",
    "assert MachineLearningModel(X, y)\n",
    "\n",
    "mlmodel = MachineLearningModel(X, y)\n",
    "\n",
    "# test get_data\n",
    "assert mlmodel.get_data()[0].shape == X.shape\n",
    "assert mlmodel.get_data()[1][1] == y[1]\n",
    "\n",
    "# test __create_test_train_data and get_train_test_data\n",
    "assert np.ceil(\n",
    "    10 * mlmodel.get_train_test_data()[-1].shape[0] / mlmodel.get_data()[1].shape[0]\n",
    ") == np.ceil(10 / mlmodel.n_splits)\n",
    "\n",
    "# test set_data (you should be able to change the model data completely)\n",
    "assert (\n",
    "    MachineLearningModel(X, y)  # create model as usual\n",
    "    .set_data(X.iloc[range(X.shape[0] - 1, -1, -1)], y)  # reset data in reverse order\n",
    "    .get_data()[0]  # get data\n",
    "    .iloc[0, 0]\n",
    ") == X.iloc[\n",
    "    -1, 0\n",
    "]  # first element is now reversed to last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define subclasses & functions\r\n",
    "\r\n",
    "A subclass or child class inherits all attributes and functions of a parent class, but may also have additional functions defined.\r\n",
    "\r\n",
    "Here we define an example of a subclass of `MachineLearningModel`, the `LogisticRegressionModel` which performs logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Create subclass\n",
    "# now you see, that the subclass inherits data handling functions from the base class,\n",
    "# and we do not need to redefine them (although we could if we wanted to!\n",
    "\n",
    "\n",
    "class LogisticRegressionClassifier(MachineLearningModel):\n",
    "    \"\"\"\n",
    "    Logistic regression classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, n_splits=5, seed=0):\n",
    "\n",
    "        # we need to initialize the parent class with super.init:\n",
    "        super(LogisticRegressionClassifier, self).__init__(\n",
    "            X, y, n_splits=n_splits, seed=seed\n",
    "        )\n",
    "\n",
    "        # define preprocessing, algorithm and pipe\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = LogisticRegression()\n",
    "        self.pipe = Pipeline([(\"scaler\", self.scaler), (\"estimator\", self.model)])\n",
    "\n",
    "        # cross validation for optimization\n",
    "        self.cv = StratifiedKFold(n_splits=self.n_splits)\n",
    "\n",
    "        # param grid for optimization\n",
    "        self.param_grid = {\n",
    "            \"estimator__C\": np.linspace(0.3, 1.7, 10)  # logspace(-4, 4, 10),\n",
    "        }\n",
    "\n",
    "        # define optimization method for optimizing the model\n",
    "        self.optimization_pipe = GridSearchCV(\n",
    "            estimator=self.pipe,\n",
    "            param_grid=self.param_grid,\n",
    "            scoring=\"accuracy\",\n",
    "            cv=self.cv,\n",
    "            return_train_score=True,\n",
    "        )\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        Train and evaluate model\n",
    "        \"\"\"\n",
    "        if X is None or y is None:\n",
    "            self.pipe.fit(self.X_train, self.y_train)\n",
    "        else:  # reset data, recreate training and testing data and recursively call fit\n",
    "            self.set_data(X, y).fit()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Get predicted value at X\n",
    "        \"\"\"\n",
    "        return self.pipe.predict(X)\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Return loss (model quality metric) [f1 score]\n",
    "\n",
    "        Note that this may be a different metric than the one that the model optimizer is using (scoring method).\n",
    "        For example for LogisticRegression the scoring method is mean accuracy,\n",
    "        but we want to track f1-score for loss because it is better balanced.\n",
    "        \"\"\"\n",
    "\n",
    "        return f1_score(y, self.predict(X))\n",
    "\n",
    "    def get_train_loss(self):\n",
    "        \"\"\"\n",
    "        Return loss for training data\n",
    "        \"\"\"\n",
    "        return self.loss(self.X_train, self.y_train)\n",
    "\n",
    "    def get_test_loss(self):\n",
    "        \"\"\"\n",
    "        Return loss for testing data\n",
    "        \"\"\"\n",
    "        return self.loss(self.X_test, self.y_test)\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Optimize model hyperparameters and fit the model with optimized parameters.\n",
    "\n",
    "        This example is with GridSearchCV, but more efficient algorithms can be implemented in practice.\n",
    "        \"\"\"\n",
    "        self.optimization_pipe.fit(self.X_train, self.y_train)\n",
    "        self.pipe.set_params(\n",
    "            estimator__C=self.optimization_pipe.best_params_[\"estimator__C\"]\n",
    "        )\n",
    "        self.fit()\n",
    "        return self\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Return parameters\n",
    "        \"\"\"\n",
    "        return self.pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit test subclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test init\n",
    "assert LogisticRegressionClassifier(X, y)\n",
    "lgr_model = LogisticRegressionClassifier(X, y)\n",
    "\n",
    "# test fit\n",
    "try:  # fit should be called before predict or score\n",
    "    lgr_model.predict([1, 1, 1, 1])  # so this will cause an error\n",
    "except:  # but the except statement catches the error\n",
    "    pass  # yes, you can also test what should not work!\n",
    "\n",
    "# there are two ways we can call the fit function: with and without data\n",
    "assert lgr_model.fit()\n",
    "assert lgr_model.fit(X, y)\n",
    "\n",
    "# test predict\n",
    "assert lgr_model.predict(X[::1]).any()\n",
    "# test loss\n",
    "assert lgr_model.get_train_loss()\n",
    "assert lgr_model.get_test_loss()\n",
    "\n",
    "# test get_params\n",
    "assert lgr_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we might observe that our model is very slow (this example is not).\r\n",
    "\r\n",
    "Then, we could try to evaluate how much time each of the functions,\r\n",
    "and even the contents of the functions take to identify the bottlenecks.\r\n",
    "The theory of [order of functions](https://en.wikipedia.org/wiki/Big_O_notation) may also be useful.\r\n",
    "\r\n",
    "However, this is something you should only do in the late stages of your project.\r\n",
    "Remember, thinking time is what matters in data science!\r\n",
    "Begin with overoptimizing things, and you'll never have results.\r\n",
    "\r\n",
    "Anyway, you can easily time functions in notebooks with `%%timeit` [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html):\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%timeit` not found.\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "%%timeit\n",
    "LogisticRegressionClassifier(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize model behaviour with toy data\r\n",
    "\r\n",
    "Now, with the unit tests we can assume that our model does something right.\r\n",
    "At least any of the functions do not crash with expected input.\r\n",
    "\r\n",
    "As with the data preparation, the last step is to visualize the model performance.\r\n",
    "With small sample data, we may not see anything interestin,\r\n",
    "but sometimes already small number of datapoints can reveal interesting properties of the model when visualized.\r\n",
    "\r\n",
    "You can also define functions for visualizing the model performance, and export them to the model module,\r\n",
    "or include them directly as part of your machine learning model class if you see benefits from it.\r\n",
    "Either way, it's better to test them too with the toy data before the real deal!\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite often we would like to see how a model performs when the number of data is increased.\r\n",
    "The two common questions are:\r\n",
    "\r\n",
    "1. How much data is needed that the model is accurate?\r\n",
    "\r\n",
    "2. How much data we can put in the model and still be able to run it with our resources?\r\n",
    "\r\n",
    "Then, we might have to balance between these two.\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "So, for our example, let's loop through a range of data points, fit and time the model at each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/templateenv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "/anaconda/envs/templateenv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "/anaconda/envs/templateenv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "/anaconda/envs/templateenv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    }
   ],
   "source": [
    "import time  # library for checking time\n",
    "\n",
    "lgr = LogisticRegressionClassifier(X, y)\n",
    "\n",
    "# dataframe to save results\n",
    "eval_df = pd.DataFrame()\n",
    "\n",
    "for i in range(10, X.shape[0]):\n",
    "    lgr.fit(X.iloc[:i], y.iloc[:i])\n",
    "\n",
    "    begin = time.time()  # measure time before model is optimized\n",
    "    lgr.optimize()\n",
    "    end = time.time()  # measure time after optimization\n",
    "\n",
    "    ret = pd.DataFrame(\n",
    "        {\n",
    "            \"round\": [i - 10],\n",
    "            \"n_obs\": [lgr.get_data()[0].shape[0]],\n",
    "            \"train_loss\": [lgr.get_train_loss()],\n",
    "            \"test_loss\": [lgr.get_test_loss()],\n",
    "            \"optimized_C\": [lgr.get_params()[\"estimator__C\"]],\n",
    "            \"optimization_time\": end - begin,  # time spent in optimization\n",
    "        }\n",
    "    )\n",
    "    eval_df = pd.concat([eval_df, ret], axis=0, ignore_index=True)\n",
    "eval_df.set_index(\"round\", inplace=True)\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the stratification gives some warnings with such a small test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can also visualize the results. Super simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_df.drop(\"n_obs\", axis=1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in you application you might be interested in completely different measures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of this notebook\r\n",
    "\r\n",
    "The result of this notebook is a collection methods ready for evaluation with the real data.\r\n",
    "\r\n",
    "You should export classes and functions to `model.py` with `# nbdev_build_lib` (workflows will do this automatically)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can move on to loss notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (templateenv)",
   "language": "python",
   "name": "templateenv_py3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
